{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(3)\n",
        "\n",
        "# Number of inputs (tokens)\n",
        "N = 3\n",
        "# Dimensionality of each input\n",
        "D = 4\n",
        "\n",
        "# Generate random inputs — list of (D, 1) column vectors\n",
        "all_x = [np.random.normal(size=(D, 1)) for _ in range(N)]\n",
        "print(all_x)\n",
        "\n",
        "# Set random seed again for reproducibility of parameters\n",
        "np.random.seed(0)\n",
        "\n",
        "# Initialize weight matrices for query, key, value projections\n",
        "omega_q = np.random.normal(size=(D, D))\n",
        "omega_k = np.random.normal(size=(D, D))\n",
        "omega_v = np.random.normal(size=(D, D))\n",
        "\n",
        "# Initialize bias vectors for query, key, value projections\n",
        "beta_q = np.random.normal(size=(D, 1))\n",
        "beta_k = np.random.normal(size=(D, 1))\n",
        "beta_v = np.random.normal(size=(D, 1))\n",
        "\n",
        "# Compute queries, keys, and values for each input\n",
        "all_queries, all_keys, all_values = [], [], []\n",
        "for x in all_x:\n",
        "    query = omega_q @ x + beta_q    # Linear transformation for query\n",
        "    key = omega_k @ x + beta_k      # Linear transformation for key\n",
        "    value = omega_v @ x + beta_v    # Linear transformation for value\n",
        "    all_queries.append(query)\n",
        "    all_keys.append(key)\n",
        "    all_values.append(value)\n",
        "\n",
        "# Softmax function to convert scores to probabilities\n",
        "def softmax(items_in):\n",
        "    e_x = np.exp(items_in - np.max(items_in))  # Subtract max for numerical stability\n",
        "    return e_x / e_x.sum()                     # Normalize values to sum to 1\n",
        "\n",
        "# Compute attention outputs individually\n",
        "all_x_prime = []\n",
        "for n in range(N):\n",
        "    q_n = all_queries[n]                      # Query for nth output\n",
        "    all_km_qn = [float(k.T @ q_n) for k in all_keys]  # Dot products of q_n with all keys\n",
        "    attention = softmax(all_km_qn)            # Compute attention weights\n",
        "    print(f\"Attentions for output {n}:\\n{attention}\")\n",
        "\n",
        "    # Weighted sum of values using attention weights (Equation 12.3)\n",
        "    x_prime = sum(attention[m] * all_values[m] for m in range(N))\n",
        "    all_x_prime.append(x_prime)\n",
        "\n",
        "# Display results of manual self-attention\n",
        "for i, x_p in enumerate(all_x_prime):\n",
        "    print(f\"x_prime_{i}_calculated:\", x_p.T)\n",
        "\n",
        "# Self-attention in matrix form (simplified batch computation)\n",
        "def self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "    Q = omega_q @ X + beta_q        # Compute all queries at once (D, N)\n",
        "    K = omega_k @ X + beta_k        # Compute all keys at once (D, N)\n",
        "    V = omega_v @ X + beta_v        # Compute all values at once (D, N)\n",
        "\n",
        "    dot_products = Q.T @ K          # Dot product between queries and keys (N, N)\n",
        "    attention = np.apply_along_axis(softmax, 1, dot_products)  # Softmax on each query row\n",
        "\n",
        "    X_prime = (attention @ V.T).T   # Weight values by attention scores\n",
        "    return X_prime\n",
        "\n",
        "# Stack input vectors into a matrix (D, N)\n",
        "X = np.hstack(all_x)\n",
        "\n",
        "# Run matrix-based self-attention\n",
        "X_prime = self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "print(\"Matrix form self-attention result:\\n\", X_prime)\n",
        "\n",
        "# Scaled dot-product self-attention\n",
        "def scaled_dot_product_self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "    Q = omega_q @ X + beta_q        # Compute queries\n",
        "    K = omega_k @ X + beta_k        # Compute keys\n",
        "    V = omega_v @ X + beta_v        # Compute values\n",
        "\n",
        "    scale = np.sqrt(D)              # Scale factor (√D) for dot products\n",
        "    dot_products = (Q.T @ K) / scale  # Scaled dot product\n",
        "    attention = np.apply_along_axis(softmax, 1, dot_products)  # Softmax per query\n",
        "\n",
        "    X_prime = (attention @ V.T).T   # Weighted sum of values\n",
        "    return X_prime\n",
        "\n",
        "# Run scaled dot-product self-attention\n",
        "X_prime_scaled = scaled_dot_product_self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "print(\"Scaled dot-product attention result:\\n\", X_prime_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuGDRbbgO3We",
        "outputId": "46f8cac5-3111-4bb0-db7f-43d0c9eb797d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[ 1.78862847],\n",
            "       [ 0.43650985],\n",
            "       [ 0.09649747],\n",
            "       [-1.8634927 ]]), array([[-0.2773882 ],\n",
            "       [-0.35475898],\n",
            "       [-0.08274148],\n",
            "       [-0.62700068]]), array([[-0.04381817],\n",
            "       [-0.47721803],\n",
            "       [-1.31386475],\n",
            "       [ 0.88462238]])]\n",
            "Attentions for output 0:\n",
            "[1.24326146e-13 9.98281489e-01 1.71851130e-03]\n",
            "Attentions for output 1:\n",
            "[2.79525306e-12 5.85506360e-03 9.94144936e-01]\n",
            "Attentions for output 2:\n",
            "[0.00505708 0.00654776 0.98839516]\n",
            "x_prime_0_calculated: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
            "x_prime_1_calculated: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
            "x_prime_2_calculated: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n",
            "Matrix form self-attention result:\n",
            " [[ 0.94744244  1.64201168  1.61949281]\n",
            " [-0.24348429 -0.08470004 -0.06641533]\n",
            " [-0.91310441  4.02764044  3.96863308]\n",
            " [-0.44522983  2.18690791  2.15858316]]\n",
            "Scaled dot-product attention result:\n",
            " [[ 0.97411966  1.59622051  1.32638014]\n",
            " [-0.23738409 -0.09516106  0.13062402]\n",
            " [-0.72333202  3.70194096  3.02371664]\n",
            " [-0.34413007  2.01339538  1.6902419 ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-a7290f8ed4b7>:48: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  all_km_qn = [float(k.T @ q_n) for k in all_keys]  # Dot products of q_n with all keys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3oIRA5knO3Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-head Attention"
      ],
      "metadata": {
        "id": "hoVEETvP8o_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(3)\n",
        "\n",
        "# Number of inputs (tokens)\n",
        "N = 6\n",
        "# Dimensionality of each input vector\n",
        "D = 8\n",
        "\n",
        "# Input matrix X of shape (D, N)\n",
        "X = np.random.normal(size=(D, N))\n",
        "print(\"Input X:\")\n",
        "print(X)\n",
        "\n",
        "# Number of attention heads\n",
        "H = 2\n",
        "# Dimensionality per head\n",
        "H_D = D // H\n",
        "\n",
        "# Set seed again for reproducibility of weights\n",
        "np.random.seed(0)\n",
        "\n",
        "# Parameters for Head 1\n",
        "omega_q1 = np.random.normal(size=(H_D, D))\n",
        "omega_k1 = np.random.normal(size=(H_D, D))\n",
        "omega_v1 = np.random.normal(size=(H_D, D))\n",
        "beta_q1 = np.random.normal(size=(H_D, 1))\n",
        "beta_k1 = np.random.normal(size=(H_D, 1))\n",
        "beta_v1 = np.random.normal(size=(H_D, 1))\n",
        "\n",
        "# Parameters for Head 2\n",
        "omega_q2 = np.random.normal(size=(H_D, D))\n",
        "omega_k2 = np.random.normal(size=(H_D, D))\n",
        "omega_v2 = np.random.normal(size=(H_D, D))\n",
        "beta_q2 = np.random.normal(size=(H_D, 1))\n",
        "beta_k2 = np.random.normal(size=(H_D, 1))\n",
        "beta_v2 = np.random.normal(size=(H_D, 1))\n",
        "\n",
        "# Final linear projection parameters\n",
        "omega_c = np.random.normal(size=(D, D))\n",
        "\n",
        "# Define softmax operation that works independently on each column\n",
        "def softmax_cols(data_in):\n",
        "  # Exponentiate all of the values\n",
        "  exp_values = np.exp(data_in) ;\n",
        "  # Sum over columns\n",
        "  denom = np.sum(exp_values, axis = 0);\n",
        "  # Compute softmax (numpy broadcasts denominator to all rows automatically)\n",
        "  softmax = exp_values / denom\n",
        "  # return the answer\n",
        "  return softmax\n",
        "\n",
        "# Multi-head scaled dot-product self-attention function\n",
        "def multihead_scaled_self_attention(X,\n",
        "                                     omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1,\n",
        "                                     omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2,\n",
        "                                     omega_c):\n",
        "    D, N = X.shape\n",
        "    H_D = D // 2  # Head dimension\n",
        "\n",
        "    # Head 1\n",
        "    Q1 = omega_q1 @ X + beta_q1\n",
        "    K1 = omega_k1 @ X + beta_k1\n",
        "    V1 = omega_v1 @ X + beta_v1\n",
        "\n",
        "    scores1 = (Q1.T @ K1) / np.sqrt(H_D)\n",
        "    weights1 = softmax_cols(scores1.T).T\n",
        "    head1_output = V1 @ weights1\n",
        "\n",
        "    # Head 2\n",
        "    Q2 = omega_q2 @ X + beta_q2\n",
        "    K2 = omega_k2 @ X + beta_k2\n",
        "    V2 = omega_v2 @ X + beta_v2\n",
        "\n",
        "    scores2 = (Q2.T @ K2) / np.sqrt(H_D)\n",
        "    weights2 = softmax_cols(scores2.T).T\n",
        "    head2_output = V2 @ weights2\n",
        "\n",
        "    # Concatenate outputs from both heads\n",
        "    concat_heads = np.vstack((head1_output, head2_output))\n",
        "\n",
        "    # Final linear projection\n",
        "    X_prime = omega_c @ concat_heads\n",
        "    return X_prime\n",
        "\n",
        "# Compute the output using multi-head self-attention\n",
        "X_prime = multihead_scaled_self_attention(\n",
        "    X, omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1,\n",
        "    omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2,\n",
        "    omega_c\n",
        ")\n",
        "\n",
        "# Display results\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "print(\"\\nYour answer:\")\n",
        "print(X_prime)\n",
        "\n",
        "print(\"\\nTrue values:\")\n",
        "print(\"[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\")\n",
        "print(\" [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\")\n",
        "print(\" [  5.479   1.115   9.244   0.453   5.656   7.089]\")\n",
        "print(\" [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\")\n",
        "print(\" [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\")\n",
        "print(\" [  3.548  10.036  -2.244   1.604  12.113  -2.557]\")\n",
        "print(\" [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]]\")\n"
      ],
      "metadata": {
        "id": "RpZxXNi7O3Qc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1acf008c-149f-48c2-ac80-1d726fefabd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input X:\n",
            "[[ 1.789  0.437  0.096 -1.863 -0.277 -0.355]\n",
            " [-0.083 -0.627 -0.044 -0.477 -1.314  0.885]\n",
            " [ 0.881  1.71   0.05  -0.405 -0.545 -1.546]\n",
            " [ 0.982 -1.101 -1.185 -0.206  1.486  0.237]\n",
            " [-1.024 -0.713  0.625 -0.161 -0.769 -0.23 ]\n",
            " [ 0.745  1.976 -1.244 -0.626 -0.804 -2.419]\n",
            " [-0.924 -1.024  1.124 -0.132 -1.623  0.647]\n",
            " [-0.356 -1.743 -0.597 -0.589 -0.874  0.03 ]]\n",
            "\n",
            "Your answer:\n",
            "[[ -6.116  -2.101  -4.916   1.423 -23.905   1.114]\n",
            " [ -0.292  11.13   -3.113   3.138   1.478  -1.398]\n",
            " [  3.321  -2.467  -5.165   9.575  19.153  -6.726]\n",
            " [ -0.572 -16.139  -6.007   3.42    2.422  -2.753]\n",
            " [ -3.825  -8.241  -1.443   0.117 -15.956   1.291]\n",
            " [  1.683   0.325  -4.154   0.296   3.734 -16.751]\n",
            " [ -1.102   7.947  10.164  -7.631  -5.134  17.329]\n",
            " [  3.313   8.716 -11.429  12.2    16.57  -19.903]]\n",
            "\n",
            "True values:\n",
            "[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\n",
            " [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\n",
            " [  5.479   1.115   9.244   0.453   5.656   7.089]\n",
            " [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\n",
            " [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\n",
            " [  3.548  10.036  -2.244   1.604  12.113  -2.557]\n",
            " [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_OZDmsB_c4v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}